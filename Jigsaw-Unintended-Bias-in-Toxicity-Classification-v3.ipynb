{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from string import punctuation\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59848</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59849</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59852</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59856</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                       comment_text  \\\n",
       "id                                                                   \n",
       "59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "59855  0.000000  Is this something I'll be able to install on m...   \n",
       "59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack   insult  threat  asian  \\\n",
       "id                                                                         \n",
       "59848         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59849         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59852         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59855         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59856         0.021277      0.0         0.021277  0.87234     0.0    0.0   \n",
       "\n",
       "       atheist  bisexual            ...             article_id    rating  \\\n",
       "id                                  ...                                    \n",
       "59848      NaN       NaN            ...                   2006  rejected   \n",
       "59849      NaN       NaN            ...                   2006  rejected   \n",
       "59852      NaN       NaN            ...                   2006  rejected   \n",
       "59855      NaN       NaN            ...                   2006  rejected   \n",
       "59856      0.0       0.0            ...                   2006  rejected   \n",
       "\n",
       "       funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "id                                                         \n",
       "59848      0    0    0      0         0              0.0   \n",
       "59849      0    0    0      0         0              0.0   \n",
       "59852      0    0    0      0         0              0.0   \n",
       "59855      0    0    0      0         0              0.0   \n",
       "59856      0    0    0      1         0              0.0   \n",
       "\n",
       "       identity_annotator_count  toxicity_annotator_count  \n",
       "id                                                         \n",
       "59848                         0                         4  \n",
       "59849                         0                         4  \n",
       "59852                         0                         4  \n",
       "59855                         0                         4  \n",
       "59856                         4                        47  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"data/train.csv\", index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray = df.values\n",
    "X = nparray[:,1]   #comment_text column\n",
    "Y = nparray[:, 0]   #target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape -  (1804874,)\n",
      "Y shape -  (1804874,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape - \", X.shape)\n",
    "print(\"Y shape - \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n",
       "       \"Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\",\n",
       "       'This is such an urgent design problem; kudos to you for taking it on. Very impressive!',\n",
       "       ..., 'thank you ,,,right or wrong,,, i am following your advice',\n",
       "       'Anyone who is quoted as having the following exchange, even if apocryphal, would have received my vote! \\n\\nBessie Braddock: \"Winston, you are drunk, and whatâ€™s more you are disgustingly drunk.\"\\nWinston Churchill: \"Bessie, my dear, you are ugly, and whatâ€™s more, you are disgustingly ugly. But tomorrow I shall be sober and you will still be disgustingly ugly.\"',\n",
       "       'Students defined as EBD are legally just as disabled and eligible for special services as a developmentally disabled or physically disabled student. \\n\\nEMOTIONAL AND BEHAVIORAL DISORDER (EBD).\\nDefinition. \\nAn emotional and behavioral disorder is an emotional disability characterized by the following: \\n(i)  An inability to build or maintain satisfactory interpersonal relationships with peers and/or teachers.  For preschool-age children, this would include \\nother care providers.  \\n(ii) An inability to learn which cannot be adequately explained by intellectual,sensory or health factors.  \\n(iii) A consistent or chronic inappropriate type of behavior or feelings under normal conditions. \\n(iv) A displayed pervasive mood of unhappiness or depression. \\n(v)  A displayed tendency to develop physical symptoms, pains or unreasonable fears associated with personal or school problem\\ns.  [34 C.F.R. Â§ 300.8(c)(4)(i)(A â€“ E)]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0, 0.0, ..., 0.0, 0.6212121212121212, 0.0], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "X_train shape -  (1786825,)\n",
      "Y_train shape -  (1786825,)\n",
      "Test data\n",
      "X_test shape -  (18049,)\n",
      "Y_test shape -  (18049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\")\n",
    "print(\"X_train shape - \", X_train.shape)\n",
    "print(\"Y_train shape - \", Y_train.shape)\n",
    "print(\"Test data\")\n",
    "print(\"X_test shape - \", X_test.shape)\n",
    "print(\"Y_test shape - \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words, words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word_to_index -  400000\n",
      "length of word_to_vec_map -  400000\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word_to_index - \", len(word_to_index))\n",
    "print(\"length of word_to_vec_map - \", len(word_to_vec_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence = strip_punctuation(X[i])\n",
    "        sentence_words =sentence.lower().split()\n",
    "        filtered_words = [word for word in sentence_words if word in vocabulary]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in filtered_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            #Truncate the sentences with maxlength\n",
    "            if j == max_len - 1:\n",
    "                break;\n",
    "            # Increment j to j + 1\n",
    "            j = j+1\n",
    "            \n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    #X = LSTM(128, return_sequences=True)(c)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(rate=0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    #X = LSTM(128)(X)\n",
    "    X = LSTM(128, activation='sigmoid')(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('sigmoid')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=[sentence_indices], outputs=[X])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 128, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,091,827\n",
      "Trainable params: 91,777\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#maxLen = len(max(X_train, key=len).split())\n",
    "\n",
    "#use 90 percentile length instead of maxlength to reduce number of steps. For this problem maxlen = 317 90thpercentile = 128\n",
    "length_list = [len(x.split()) for x in X_train]\n",
    "maxLen = np.percentile(length_list, 90)\n",
    "maxLen = int(maxLen)\n",
    "#print(np.percentile(length_list, 90))\n",
    "\n",
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[357266., 166369., 216139., 237534., 329916., 188287., 269798.,\n",
       "        357266., 146233., 287877., 114253., 334274., 269798., 288009.,\n",
       "        360915., 129404.,  43010., 162321., 268046., 371755., 216139.,\n",
       "        383772., 269798.,  43010., 115975.,  69709.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.],\n",
       "       [123517., 394475., 173061., 360915., 302207.,  51582., 357266.,\n",
       "        343907.,  44608., 366071.,  72182., 357266., 148908., 287045.,\n",
       "        360915.,  71090., 192126., 185457., 168567., 394475., 246267.,\n",
       "        357212., 225629., 193716., 373317.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.],\n",
       "       [360915., 272881., 177443., 278468., 291753.,  54718., 155653.,\n",
       "        357152., 394475., 366071.,  91041., 141816., 319691., 298728.,\n",
       "        193297., 270970.,  46704., 357652., 346373., 357959.,  60665.,\n",
       "        193315., 394475., 366208., 178965., 360915., 231458., 118793.,\n",
       "        360915., 293803.,  90562.,  54718.,  90586., 174695.,  62065.,\n",
       "        379231., 308296., 192973., 125245.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.],\n",
       "       [357266.,  50434., 239026., 345161.,  58997., 363594., 188481.,\n",
       "         50433.,  88126.,  50434., 369415., 144015.,  54718.,  47512.,\n",
       "        283886., 357810.,  58997., 363594., 270970.,  58997., 357810.,\n",
       "        353915., 188481.,  50433., 357266., 123763., 192973.,  80586.,\n",
       "        219115., 222471., 270193., 102220., 377881., 172655., 134009.,\n",
       "        313432., 312095., 292835., 140213., 185457., 110156.,  71090.,\n",
       "        246351., 126852., 165582., 154323.,  43010., 239026., 293158.,\n",
       "        306173., 290454., 333817.,  54718., 384374., 127708., 174642.,\n",
       "         43010., 143937., 188481.,  50433., 360915., 106967., 358452.,\n",
       "        290458.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.],\n",
       "       [155771.,  43010., 302801., 125829., 110554., 313211., 379657.,\n",
       "        229835.,  51582., 207765., 268046.,  78853., 222999., 188903.,\n",
       "        277915., 268046., 357266., 303140., 342019., 258200., 377458.,\n",
       "        141948., 126848., 193716.,  54718., 192973., 262350., 225401.,\n",
       "        107670.,  43010., 379436., 268046., 140300., 231458., 268046.,\n",
       "        357212., 386307., 394475., 387696., 292006., 315103., 357212.,\n",
       "        384374., 270201., 106302.,  44608., 357266., 272583., 169725.,\n",
       "        209655., 141948., 126848., 193716.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 0.7161 - acc: 0.0021\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 24s 2ms/step - loss: 0.6935 - acc: 0.0021\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 23s 2ms/step - loss: 0.6934 - acc: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x523ece48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network only for a small sample - DUBUG PURPOSES\n",
    "\n",
    "X_train_indices = X_train_indices[0:10000]\n",
    "Y_train = Y_train[0:10000]\n",
    "model.fit(X_train_indices, Y_train, epochs = 3, batch_size = 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 723us/step\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_train_indices, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy =  0.0021\n",
      "Train Loss =  0.693267527961731\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy = \" , train_accuracy)\n",
    "print(\"Train Loss = \" , train_loss)\n",
    "#model.save_weights(\"lstm_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 789us/step\n",
      "\n",
      "Test accuracy =  0.002\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test[0:1000]\n",
    "Y_test = Y_test[0:1000]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = model.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('pred_output_2layer_lstm_0427.csv', mode='w') as pred_output:\n",
    "    output_writer = csv.writer(pred_output, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for i in range(Y_test.shape[0]):\n",
    "        output_writer.writerow([X_test[i].encode(\"utf-8\"), Y_test[i], Y_test_pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_5', 'dropout_6', 'embedding_8', 'input_6', 'lstm_7']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "f = h5py.File('lstm_model_1.h5', 'r')\n",
    "print(list(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)\n",
    "weights[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [-0.58402 ,  0.39031 ,  0.65282 , ..., -1.2338  ,  0.46715 ,\n",
       "         0.78858 ],\n",
       "       [-1.0588  ,  0.26952 ,  0.94632 , ...,  0.31138 ,  0.79843 ,\n",
       "         0.20392 ],\n",
       "       ...,\n",
       "       [ 0.32301 ,  0.32305 , -0.3136  , ...,  1.8267  , -0.43051 ,\n",
       "        -1.1958  ],\n",
       "       [ 0.38715 ,  0.098414, -0.73857 , ...,  1.0611  , -0.2125  ,\n",
       "        -0.65514 ],\n",
       "       [-0.019789, -0.040119,  0.86891 , ...,  0.23708 , -0.18194 ,\n",
       "         0.24766 ]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
