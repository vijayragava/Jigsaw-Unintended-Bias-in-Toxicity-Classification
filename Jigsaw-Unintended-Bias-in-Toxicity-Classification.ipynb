{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "\n",
    "import csv\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59848</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59849</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59852</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59856</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                       comment_text  \\\n",
       "id                                                                   \n",
       "59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "59855  0.000000  Is this something I'll be able to install on m...   \n",
       "59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack   insult  threat  asian  \\\n",
       "id                                                                         \n",
       "59848         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59849         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59852         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59855         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59856         0.021277      0.0         0.021277  0.87234     0.0    0.0   \n",
       "\n",
       "       atheist  bisexual            ...             article_id    rating  \\\n",
       "id                                  ...                                    \n",
       "59848      NaN       NaN            ...                   2006  rejected   \n",
       "59849      NaN       NaN            ...                   2006  rejected   \n",
       "59852      NaN       NaN            ...                   2006  rejected   \n",
       "59855      NaN       NaN            ...                   2006  rejected   \n",
       "59856      0.0       0.0            ...                   2006  rejected   \n",
       "\n",
       "       funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "id                                                         \n",
       "59848      0    0    0      0         0              0.0   \n",
       "59849      0    0    0      0         0              0.0   \n",
       "59852      0    0    0      0         0              0.0   \n",
       "59855      0    0    0      0         0              0.0   \n",
       "59856      0    0    0      1         0              0.0   \n",
       "\n",
       "       identity_annotator_count  toxicity_annotator_count  \n",
       "id                                                         \n",
       "59848                         0                         4  \n",
       "59849                         0                         4  \n",
       "59852                         0                         4  \n",
       "59855                         0                         4  \n",
       "59856                         4                        47  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"data/train.csv\", index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray = df.values\n",
    "X_train = nparray[:,1]   #comment_text column\n",
    "Y_train = nparray[:, 0]   #target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n",
       "       \"Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\",\n",
       "       'This is such an urgent design problem; kudos to you for taking it on. Very impressive!',\n",
       "       ..., 'thank you ,,,right or wrong,,, i am following your advice',\n",
       "       'Anyone who is quoted as having the following exchange, even if apocryphal, would have received my vote! \\n\\nBessie Braddock: \"Winston, you are drunk, and what’s more you are disgustingly drunk.\"\\nWinston Churchill: \"Bessie, my dear, you are ugly, and what’s more, you are disgustingly ugly. But tomorrow I shall be sober and you will still be disgustingly ugly.\"',\n",
       "       'Students defined as EBD are legally just as disabled and eligible for special services as a developmentally disabled or physically disabled student. \\n\\nEMOTIONAL AND BEHAVIORAL DISORDER (EBD).\\nDefinition. \\nAn emotional and behavioral disorder is an emotional disability characterized by the following: \\n(i)  An inability to build or maintain satisfactory interpersonal relationships with peers and/or teachers.  For preschool-age children, this would include \\nother care providers.  \\n(ii) An inability to learn which cannot be adequately explained by intellectual,sensory or health factors.  \\n(iii) A consistent or chronic inappropriate type of behavior or feelings under normal conditions. \\n(iv) A displayed pervasive mood of unhappiness or depression. \\n(v)  A displayed tendency to develop physical symptoms, pains or unreasonable fears associated with personal or school problem\\ns.  [34 C.F.R. § 300.8(c)(4)(i)(A – E)]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0, 0.0, ..., 0.0, 0.6212121212121212, 0.0], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words =X[i].lower().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j+1\n",
    "            \n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(X, word_to_vec_map):\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]    #dimension = 50\n",
    "    \n",
    "    X_averages = np.zeros((m,emb_dim))\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
    "        sentence = strip_punctuation(X[i])\n",
    "        words = sentence.lower().split()\n",
    "\n",
    "        # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "        \n",
    "        vec_sum = np.zeros(emb_dim)\n",
    "    \n",
    "        # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "        for w in words:\n",
    "            vec_sum += word_to_vec_map[w]\n",
    "        \n",
    "        avg = vec_sum/len(words)\n",
    "            \n",
    "        X_averages[i] = avg\n",
    "        \n",
    "        return X_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_averages = np.zeros((10,3))\n",
    "X_averages\n",
    "#word_to_vec_map[\"cucumber\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_averages = sentence_to_avg(X_train, word_to_vec_map)    # this is of shape (1804874, 50)\n",
    "#X_averages = np.transpose(X_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 50)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_averages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_train = Y_train.reshape(1, Y_train.shape[0])\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1804874/1804874 [==============================] - 32s 18us/step - loss: 0.3480 - acc: 0.7007\n",
      "Epoch 2/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 3/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 4/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 5/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 6/50\n",
      "1804874/1804874 [==============================] - 32s 18us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 7/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 8/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 9/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 10/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 11/50\n",
      "1804874/1804874 [==============================] - 32s 18us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 12/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 13/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 14/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 15/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 16/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 17/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 18/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 19/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 20/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 21/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 22/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 23/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 24/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 25/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 26/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 27/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 28/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 29/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 30/50\n",
      "1804874/1804874 [==============================] - 30s 16us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 31/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 32/50\n",
      "1804874/1804874 [==============================] - 33s 18us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 33/50\n",
      "1804874/1804874 [==============================] - 33s 18us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 34/50\n",
      "1804874/1804874 [==============================] - 33s 19us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 35/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 36/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 37/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 38/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 39/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 40/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 41/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 42/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 43/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 44/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 45/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 46/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 47/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 48/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 49/50\n",
      "1804874/1804874 [==============================] - 31s 17us/step - loss: 0.3317 - acc: 0.7007\n",
      "Epoch 50/50\n",
      "1804874/1804874 [==============================] - 30s 17us/step - loss: 0.3317 - acc: 0.7007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a6dc3c18>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.transpose(Y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_averages, Y_train, epochs=50, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874/1804874 [==============================] - 32s 18us/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_averages, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 70.07%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
