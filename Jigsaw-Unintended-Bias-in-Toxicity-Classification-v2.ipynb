{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from string import punctuation\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59848</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59849</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59852</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59856</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                       comment_text  \\\n",
       "id                                                                   \n",
       "59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "59855  0.000000  Is this something I'll be able to install on m...   \n",
       "59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack   insult  threat  asian  \\\n",
       "id                                                                         \n",
       "59848         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59849         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59852         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59855         0.000000      0.0         0.000000  0.00000     0.0    NaN   \n",
       "59856         0.021277      0.0         0.021277  0.87234     0.0    0.0   \n",
       "\n",
       "       atheist  bisexual            ...             article_id    rating  \\\n",
       "id                                  ...                                    \n",
       "59848      NaN       NaN            ...                   2006  rejected   \n",
       "59849      NaN       NaN            ...                   2006  rejected   \n",
       "59852      NaN       NaN            ...                   2006  rejected   \n",
       "59855      NaN       NaN            ...                   2006  rejected   \n",
       "59856      0.0       0.0            ...                   2006  rejected   \n",
       "\n",
       "       funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "id                                                         \n",
       "59848      0    0    0      0         0              0.0   \n",
       "59849      0    0    0      0         0              0.0   \n",
       "59852      0    0    0      0         0              0.0   \n",
       "59855      0    0    0      0         0              0.0   \n",
       "59856      0    0    0      1         0              0.0   \n",
       "\n",
       "       identity_annotator_count  toxicity_annotator_count  \n",
       "id                                                         \n",
       "59848                         0                         4  \n",
       "59849                         0                         4  \n",
       "59852                         0                         4  \n",
       "59855                         0                         4  \n",
       "59856                         4                        47  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"data/train.csv\", index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparray = df.values\n",
    "X = nparray[:,1]   #comment_text column\n",
    "Y = nparray[:, 0]   #target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape -  (1804874,)\n",
      "Y shape -  (1804874,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape - \", X.shape)\n",
    "print(\"Y shape - \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n",
       "       \"Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\",\n",
       "       'This is such an urgent design problem; kudos to you for taking it on. Very impressive!',\n",
       "       ..., 'thank you ,,,right or wrong,,, i am following your advice',\n",
       "       'Anyone who is quoted as having the following exchange, even if apocryphal, would have received my vote! \\n\\nBessie Braddock: \"Winston, you are drunk, and what’s more you are disgustingly drunk.\"\\nWinston Churchill: \"Bessie, my dear, you are ugly, and what’s more, you are disgustingly ugly. But tomorrow I shall be sober and you will still be disgustingly ugly.\"',\n",
       "       'Students defined as EBD are legally just as disabled and eligible for special services as a developmentally disabled or physically disabled student. \\n\\nEMOTIONAL AND BEHAVIORAL DISORDER (EBD).\\nDefinition. \\nAn emotional and behavioral disorder is an emotional disability characterized by the following: \\n(i)  An inability to build or maintain satisfactory interpersonal relationships with peers and/or teachers.  For preschool-age children, this would include \\nother care providers.  \\n(ii) An inability to learn which cannot be adequately explained by intellectual,sensory or health factors.  \\n(iii) A consistent or chronic inappropriate type of behavior or feelings under normal conditions. \\n(iv) A displayed pervasive mood of unhappiness or depression. \\n(v)  A displayed tendency to develop physical symptoms, pains or unreasonable fears associated with personal or school problem\\ns.  [34 C.F.R. § 300.8(c)(4)(i)(A – E)]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0, 0.0, ..., 0.0, 0.6212121212121212, 0.0], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "X_train shape -  (1786825,)\n",
      "Y_train shape -  (1786825,)\n",
      "Test data\n",
      "X_test shape -  (18049,)\n",
      "Y_test shape -  (18049,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\")\n",
    "print(\"X_train shape - \", X_train.shape)\n",
    "print(\"Y_train shape - \", Y_train.shape)\n",
    "print(\"Test data\")\n",
    "print(\"X_test shape - \", X_test.shape)\n",
    "print(\"Y_test shape - \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words, words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(\"data/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word_to_index -  400000\n",
      "length of word_to_vec_map -  400000\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word_to_index - \", len(word_to_index))\n",
    "print(\"length of word_to_vec_map - \", len(word_to_vec_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(X, word_to_vec_map, vocabulary):\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    #print(\"m - \", m)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]    #dimension = 50\n",
    "    \n",
    "    X_averages = np.zeros((m,emb_dim))\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
    "        sentence = strip_punctuation(X[i])\n",
    "        #print(\"sentence - \", sentence)\n",
    "        words = sentence.lower().split()\n",
    "        filtered_words = [word for word in words if word in vocabulary]\n",
    "        #print(\"filtered_words - \", filtered_words)\n",
    "        # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "        \n",
    "        vec_sum = np.zeros(emb_dim)\n",
    "    \n",
    "        # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "        for w in filtered_words:\n",
    "            vec_sum += word_to_vec_map[w]\n",
    "        \n",
    "        if (len(filtered_words) >= 1):\n",
    "            avg = vec_sum/len(filtered_words)\n",
    "            X_averages[i] = avg\n",
    "        \n",
    "        if (i % 10000 == 0):\n",
    "            print(\"Processed Entries - \", m)\n",
    "            \n",
    "    return X_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_averages = sentence_to_avg(X_train, word_to_vec_map, vocabulary)    # this is of shape (1804874, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_averages.shape -  (1786825, 50)\n"
     ]
    }
   ],
   "source": [
    "print (\"X_averages.shape - \", X_averages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1786825/1786825 [==============================] - 34s 19us/step - loss: 0.2921 - acc: 0.7006\n",
      "Epoch 2/10\n",
      "1786825/1786825 [==============================] - 33s 18us/step - loss: 0.2882 - acc: 0.7004\n",
      "Epoch 3/10\n",
      "1786825/1786825 [==============================] - 33s 18us/step - loss: 0.2875 - acc: 0.7004\n",
      "Epoch 4/10\n",
      "1786825/1786825 [==============================] - 32s 18us/step - loss: 0.2871 - acc: 0.7004\n",
      "Epoch 5/10\n",
      "1786825/1786825 [==============================] - 33s 18us/step - loss: 0.2868 - acc: 0.7004\n",
      "Epoch 6/10\n",
      "1786825/1786825 [==============================] - 33s 19us/step - loss: 0.2866 - acc: 0.7004\n",
      "Epoch 7/10\n",
      "1786825/1786825 [==============================] - 32s 18us/step - loss: 0.2865 - acc: 0.7004\n",
      "Epoch 8/10\n",
      "1786825/1786825 [==============================] - 33s 18us/step - loss: 0.2863 - acc: 0.7004\n",
      "Epoch 9/10\n",
      "1786825/1786825 [==============================] - 33s 18us/step - loss: 0.2862 - acc: 0.7004\n",
      "Epoch 10/10\n",
      "1786825/1786825 [==============================] - 33s 18us/step - loss: 0.2861 - acc: 0.7004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xea15b438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_train = np.transpose(Y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_averages, Y_train, epochs=10, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786825/1786825 [==============================] - 36s 20us/step\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate(X_averages, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy =  0.7004345696977481\n",
      "Train Loss =  0.28591722137118447\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy = \" , train_accuracy)\n",
    "print(\"Train Loss = \" , train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['httpsphotographyisnotacrimecom20160708portlandpolicearrestmanpulledextendedcliphandgunprotesters']\n",
      "['theyalwaystryiftheyhaveamajority']\n",
      "[]\n",
      "['httpwwwfreedomworksorgcontentpeterdefaziohassomeexplainingdo']\n",
      "['httpwwwthegwpfcommethanehydratemayignitenewenergywarinasia']\n",
      "['easynoyouaredescribingafascist']\n",
      "['httpsenwikipediaorgwikiolaf']\n",
      "['racsist']\n",
      "['httpswwwbrennancenterorgissuescrimeratesamerica']\n",
      "['httpwwwdoekywpcontentuploads201404sunscreencontaminationofcoralreefspdf']\n",
      "['modesly']\n",
      "['antifa']\n",
      "['𝒩ℯ𝒾𝓁', '𝒶', '𝓉𝓇𝓊ℯ', '𝒾𝓃𝓈𝓅𝒾𝓇𝒶𝓉𝒾ℴ𝓃', '𝒻ℴ𝓇', '𝓉𝒽ℴ𝓈ℯ', '𝓉𝒽𝒶𝓉', '𝓀𝓃ℴ𝓌', '𝒽ℴ𝒸𝓀ℯ𝓎']\n",
      "['httpwwwactivistpostcom201610pentagoncaughtpayingprfirm540millionmakefaketerroristvideoshtml']\n",
      "['doddfrank']\n",
      "['snorrrrrrrrrrre']\n",
      "['httpswwwpressprogresscacanadarichest1gotalotrichersincethe1980sthemiddleclasshasnot']\n",
      "['httpwwwsnopescomnytimeswiretaparticles']\n",
      "['cajones']\n",
      "['httpswwwfacebookcomdavidgrisham58', 'httpswwwfacebookcompglastfrontierevangelismaboutrefpageinternal']\n",
      "['httpnsidcorgarcticseaicenewsfiles201509monthlyice08nhpng']\n",
      "['derp']\n",
      "['yumyumpie']\n",
      "['httpswwwombalaskagovhtmlbudgetreportfy2017budgetenactedhtml']\n",
      "['httpswwwwebmdcomatozguidestcfracturedribtopicoverview1']\n",
      "['httpbeautyeditorca20140221vladimirputinbeforeandafter']\n",
      "['httpalaskanonprofitabuseblogspotcom']\n",
      "['imnotracistbutur']\n",
      "['ｔｈｅ', 'ｇｏｖｅｒｎｍｅｎｔ', 'ｌｉｅｄ', 'ｔｏ', 'ｕｓ！', '－', 'ｉｎ', '２００８', 'ｍａｎｙ', 'ｏｆ', 'ｕｓ', 'ｍａｄｅ', 'ｄｅｃｉｓｉｏｎｓ', 'ｂａｓｅｄ', 'ｏｎ', 'ｅｃｏｎｏｍｉｃ', 'ｆａｃｔｏｒｓ', 'ｔｏ', 'ｎｏｔ', 'ｂｕｙ', 'ａ', 'ｈｏｕｓｅ', 'ｗｈｉｃｈ', 'ｔｕｒｎｅｄ', 'ｏｕｔ', 'ｔｏ', 'ｂｅ', 'ａｎ', 'ｕｎｆｏｒｔｕｎａｔｅ', 'ｆｉｎａｎｃｉａｌ', 'ｄｅｃｉｓｉｏｎ．', 'ｗｈａｔ', 'ｗｅ', 'ｄｉｄｎ＇ｔ', 'ｋｎｏｗ', 'ｔｈａｔ', 'ｔｈｅ', 'ｇｏｖｅｒｎｍｅｎｔ', 'ｕｓｅｄ', 'ｏｕｒ', 'ｍｏｎｅｙ', 'ｔｏ', 'ｂａｉｌ', 'ｏｕｔ', 'ｔｈｅ', 'ｂａｎｋ', 'ｆｏｒ', '𝟭𝟭𝟰', '𝗯𝗶𝗹𝗹𝗶𝗼𝗻', '𝗱𝗼𝗹𝗹𝗮𝗿𝘀', '🇨🇦🇳🇦🇩🇮🇦🇳s', '🇨🇦🇳', '🇳🇴🇹', '🇹🇷🇺🇸🇹', '🇹🇭🇪🇮🇷', '🇴🇼🇳', '🇬🇴🇻🇪🇷🇳🇲🇪🇳🇹', '𝗵𝘁𝘁𝗽𝘀𝘆𝗼𝘂𝘁𝘂𝗯𝗲𝗴𝗞𝗭𝗘𝗳𝗿𝗪𝟱𝗺𝟳𝗤']\n",
      "['zomg']\n",
      "['superstitionbecauseofbadprooftexting']\n",
      "['theumwwillresistandruleschangesthatendangertheirmembers']\n",
      "['httpwwwelkharttruthcomnewsaskthetruth20150629askthetruth9html']\n",
      "['httpswwwyoutubecomwatchvhd4ydhzcz0s']\n",
      "['httpswwwyoutubecomwatchvuaony65vz00']\n",
      "['httpsenmwikipediaorgwikitotalitarianism']\n",
      "['httpwwwbreitbartcombiggovernment20170303marklevinobamausedpolicestatetacticsunderminetrump']\n",
      "['psychyderm']\n",
      "['asfrreesesaidblaineamendmentsareanticatholictheywillsoonfallasthiscaseisusedasprecedent', 'aslongaslocaltaxesgouptofundreligiousschoolsitisagoodthing']\n",
      "['httpwwwnytimescom20160911uswhereaboutsofcastoutpoliceofficersothercitiesoftenhirethemhtmlr0', 'httpmimesislawcomfaultlinesthedangerousshellgameofgypsycops12790', 'httpwwwcbsnewscomnewsgypsycopswithquestionablepastshiredbydifferentdepartmentslackofoversightpolice', 'httpsenwikipediaorgwikigypsycop']\n",
      "['butjudgingotherssexualityisthewideroad']\n",
      "['thepeoplewhowrotethescripturesknewaboutitindeedmarktheevangelistwasthefoundingpopeofafrica']\n",
      "['httpswwwfacebookcomgroupsgoodbyewalkerhclocationufi']\n",
      "['zzzzzzzz']\n",
      "['makehabsfansgreatagain']\n",
      "X_test_averages.shape -  (18049, 50)\n"
     ]
    }
   ],
   "source": [
    "X_test_averages = sentence_to_avg(X_test, word_to_vec_map, vocabulary)\n",
    "print (\"X_test_averages.shape - \", X_test_averages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18049/18049 [==============================] - 0s 24us/step\n",
      "Test Accuracy =  0.7002049975067871\n",
      "Test Loss =  0.2853952414164273\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_averages, Y_test)\n",
    "print(\"Test Accuracy = \" , test_accuracy)\n",
    "print(\"Test Loss = \" , test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    \n",
    "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "    \n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = model.predict(X_test_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('pred_output_0423.csv', mode='w') as pred_output:\n",
    "    output_writer = csv.writer(pred_output, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for i in range(Y_test.shape[0]):\n",
    "        output_writer.writerow([X_test[i].encode(\"utf-8\"), Y_test[i], Y_test_pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7000000</th>\n",
       "      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000001</th>\n",
       "      <td>I actually inspected the infrastructure on Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000002</th>\n",
       "      <td>No it won't . That's just wishful thinking on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000003</th>\n",
       "      <td>Instead of wringing our hands and nibbling the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000004</th>\n",
       "      <td>how many of you commenters have garbage piled ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment_text\n",
       "id                                                        \n",
       "7000000  Jeff Sessions is another one of Trump's Orwell...\n",
       "7000001  I actually inspected the infrastructure on Gra...\n",
       "7000002  No it won't . That's just wishful thinking on ...\n",
       "7000003  Instead of wringing our hands and nibbling the...\n",
       "7000004  how many of you commenters have garbage piled ..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pandas.read_csv(\"data/test.csv\", index_col=\"id\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
